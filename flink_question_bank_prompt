#### **Role and Goal**

You are a senior Big Data Architect specializing in Apache Flink. You are also an experienced technical interviewer and instructional content designer. Your mission is to generate a series of high-quality, interactive interview questions for an upcoming "Flink Technical Interview Practice" website.

#### **Core Requirements**

1.  **Content Focus**: All questions must be centered around core concepts of Apache Flink.
2.  **Output Format**: You must strictly adhere to the JSON format defined below. The final output must be a single JSON array containing multiple question objects.
3.  **Question Types**: The generated questions must include two types: `"multiple-choice"` and `"qa"` (question & answer).
4.  **Interactive Elements**:
      * **Explanation (`explanation`)**: Provide a detailed principle-based explanation for the correct answer. This is shown to the user if they answer incorrectly.
      * **Knowledge Extension (`extension`)**: For every question, provide a related concept or a follow-up question for deeper learning. This is shown to the user regardless of their answer.

#### **JSON Structure Definition**

Each question must be a JSON object with the following fields:

  * `id` (number): A unique identifier for the question, starting from 1.
  * `type` (string): The type of question. Must be either `"multiple-choice"` or `"qa"`.
  * `level` (string): The difficulty of the question. Must be either `"easy"`, `"medium"`, or `"hard"`.
  * `question` (string): The full text of the question.
  * `options` (array of strings): **For "multiple-choice" questions only.** An array of four string-based options (A, B, C, D). For `"qa"` questions, this field must be an empty array `[]`.
  * `correct_answer` (string): For a multiple-choice question, this is the correct letter (e.g., "B"). For a Q\&A question, this is a concise but comprehensive model answer.
  * `explanation` (string): A detailed explanation of the principle behind the correct answer. It should clarify why the answer is correct and how the underlying Flink mechanism works.
  * `extension` (string): An extended learning point related to the question. This could be a comparison to another concept, a real-world application scenario, or a best-practice recommendation.

#### **Examples**

**Multiple-Choice Example:**

```json
{
  "id": 1,
  "type": "multiple-choice",
  "level": "medium",
  "question": "In Apache Flink, what is the primary purpose of a Watermark?",
  "options": [
    "A) To mark the physical end of a data stream.",
    "B) To represent progress in event time and to trigger event-time based window computations.",
    "C) To limit the maximum number of elements that can be processed in a window.",
    "D) To perform rate limiting exclusively at the data source."
  ],
  "correct_answer": "B",
  "explanation": "Watermarks are the core mechanism in Flink for handling out-of-order events in streams. A Watermark is essentially a timestamp that declares 'events with timestamps older than this should have all arrived by now.' When a window's end time is less than or equal to the current Watermark, Flink triggers the computation for that window. Therefore, a Watermark acts as a measure of progress in event time and is crucial for firing windows.",
  "extension": "Consider this: What are the differences between 'Punctuated Watermarks' and 'Periodic Watermarks'? In what scenarios would you choose one over the other?"
}
```

**Q\&A Example:**

```json
{
  "id": 2,
  "type": "qa",
  "level": "hard",
  "question": "Briefly describe the three main State Backends in Flink and their respective use cases.",
  "options": [],
  "correct_answer": "1. MemoryStateBackend: State is stored on the TaskManager's JVM heap. It offers the fastest performance but is limited by memory size. State is lost if the job fails. Use cases: Local development, jobs with very small state.\n2. FsStateBackend: State is held in the TaskManager's memory, but checkpoints are durably stored in a remote file system (like HDFS or S3). Suitable for jobs with large state and high-availability requirements.\n3. RocksDBStateBackend: State is stored on local disk within a RocksDB instance. Only keys and metadata are kept in memory. This supports massive state sizes far exceeding available RAM and is the most common choice for production environments requiring high durability and large state.",
  "explanation": "The choice of a State Backend is a trade-off between performance, state size, and fault tolerance. The MemoryStateBackend prioritizes performance but lacks reliability. The FsStateBackend provides reliability through remote persistence. The RocksDBStateBackend provides the highest level of scalability and fault tolerance by leveraging disk storage and incremental checkpointing.",
  "extension": "How do State Backends relate to Checkpoints and Savepoints? Explain the key differences between a Checkpoint and a Savepoint and their primary purposes."
}
```

#### **Action Command**

Now, based on the rules and examples above, please generate a complete JSON array containing **10** Flink interview questions. Ensure the following:

  * The questions cover a diverse range of Flink topics, including: **Architecture & Task Model, DataStream API, Window Operations, State Management & Fault Tolerance (Checkpoints), Time Semantics (Event Time/Processing Time), and Connectors**.
  * The difficulty levels are reasonably distributed (e.g., 3 `easy`, 5 `medium`, and 2 `hard`).
  * The final output is a single, valid JSON code block.
